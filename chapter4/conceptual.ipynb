{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Conceptual\n",
    "1. We have 4.2 as follow:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(X) = \\frac {e^{\\beta_0 + \\beta_1 X}} {1 + e^{\\beta_0 + \\beta_1 X}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Thus we have the following:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac {p(X)} {1-p(x)} &= \\frac {\\frac {e^{\\beta_0 + \\beta_1 X}} {1 + e^{\\beta_0 + \\beta_1 X}}}\n",
    "        {1 - \\frac {e^{\\beta_0 + \\beta_1 X}} {1 + e^{\\beta_0 + \\beta_1 X}}} \\\\\n",
    "\\frac {p(X)} {1-p(x)} &= \\frac {\\frac {e^{\\beta_0 + \\beta_1 X}} {1 + e^{\\beta_0 + \\beta_1 X}}}\n",
    "        {\n",
    "          \\frac {1 + e^{\\beta_0 + \\beta_1 X}} {1 + e^{\\beta_0 + \\beta_1 X}}\n",
    "          - \\frac {e^{\\beta_0 + \\beta_1 X}} {1 + e^{\\beta_0 + \\beta_1 X}}\n",
    "        } \\\\\n",
    "\\frac {p(X)} {1-p(x)} &= \\frac {\\frac {e^{\\beta_0 + \\beta_1 X}} {1 + e^{\\beta_0 + \\beta_1 X}}}\n",
    "        {\\frac {1} {1 + e^{\\beta_0 + \\beta_1 X}}} \\\\\n",
    "\\frac {p(X)} {1-p(x)} &= e^{\\beta_0 + \\beta_1 X} \n",
    "\\end{align*}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "path = os.path.join(os.getcwd(), '1.jpg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. From 4.12 assuming $f_k(x)$ is normal or Gaussian.\n",
    "\n",
    "\\begin{align}\n",
    "p_k(x) = \\frac {\\pi_k\n",
    "                \\frac {1} {\\sqrt{2 \\pi} \\sigma}\n",
    "                \\exp(- \\frac {1} {2 \\sigma^2} (x - \\mu_k)^2)\n",
    "               }\n",
    "               {\\sum {\n",
    "                \\pi_l\n",
    "                \\frac {1} {\\sqrt{2 \\pi} \\sigma}\n",
    "                \\exp(- \\frac {1} {2 \\sigma^2} (x - \\mu_l)^2)\n",
    "               }}\n",
    "\\end{align}\n",
    "\n",
    "with 4.13\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta_k(x) = x \\frac {\\mu_k} {\\sigma^2} - \\frac {\\mu_k^2} {2 \\sigma^2}\n",
    "              + \\log(\\pi_k)\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Claim: Maximizing $p_k(x)$ is equivalent to maximizing $\\delta_k(x)$.  \n",
    "Proof: Let x remain fixed. Suppose $p_k(x) \\ge p_i(x)$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First lets focus on $p_k(x)$. We ignore the denominator since it will go away. \n",
    "Taking log of the top gives us the following\n",
    "\n",
    "\\begin{align}\n",
    "top &=-\\frac {1} {2\\mu_k^2}(x-\\mu_k)^2 + log(\\pi_k) + log(\\frac {1} {\\sqrt{2\\pi}\\sigma}) \\\\\n",
    "&= -x^2\\frac {1} {2\\sigma^2} + x\\frac {\\mu_k} {\\sigma^2} -\\frac {\\mu_k^2} {2\\sigma^2} + log(\\pi_k)\n",
    "\\end{align}\n",
    "\n",
    "Thus, based on our assumption we have:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{align}\n",
    "-\\frac {1} {2\\mu_k^2}(x-\\mu_k)^2 + log(\\pi_k) &\\ge -\\frac {1} {2\\mu_i^2}(x-\\mu_i)^2 + log(\\pi_i) \\\\\n",
    "-\\frac {x^2} {2\\sigma^2} + x\\frac {\\mu_k} {\\sigma^2} -\\frac {\\mu_k^2} {2\\sigma^2} + log(\\pi_k) &\\ge\n",
    " -\\frac {x^2} {2\\sigma^2} + x\\frac {\\mu_i} {\\sigma^2} -\\frac {\\mu_i^2} {2\\sigma^2} + log(\\pi_i) \\\\\n",
    "x\\frac {\\mu_k} {\\sigma^2} -\\frac {\\mu_k^2} {2\\sigma^2} + log(\\pi_k) &\\ge\n",
    "x\\frac {\\mu_i} {\\sigma^2} -\\frac {\\mu_i^2} {2\\sigma^2} + log(\\pi_i) \n",
    "\\end{align}\n",
    "\n",
    "The other direction has the same proof"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. The argument in 4.4.2 leads us to taking log of the $p_k(x)$ and rearrange terms. Lets do this\n",
    "for QDA $p_k(x)$. Easy enough, it is the top of the question 2. with 1 more term."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{align}\n",
    "\\delta_k(x) = -x^2\\frac {1} {2\\sigma^2} + x\\frac {\\mu_k} {\\sigma^2} -\\frac {\\mu_k^2} {2\\sigma^2} + log(\\pi_k)\n",
    "\\end{align}\n",
    "\n",
    "This function is clearly quadratic and not linear with $x^2$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Curse of dimensionality. Investigating KNN.  \n",
    "\n",
    "(a). p = 1, X is uniformly distributed on [0, 1].  \n",
    "If we using only 10% of the nearest observation to make a prediction, we only utilizes 10% of  \n",
    "data set. (ignore the <0.05 and > 0.95 case).\n",
    "we can see that   \n",
    "$E(\\alpha) = \\frac {0.1}{1} = 0.1$\n",
    "\n",
    "(b). p = 2, $(X_1, X_2) \\sim [0, 1][0, 1]$.  \n",
    "Again, if we use only 10% of the $X_1$ and 10% of $X_2$, then only 1% of the data is used!  \n",
    "$E(\\alpha) = \\frac {0.1^2}{1^2} = 0.01$\n",
    "\n",
    "(c). Similarly with p = 100  \n",
    "$E(\\alpha) = \\frac {0.1^100}{1^100} = 10^{-100} $\n",
    "\n",
    "(d) As p is increasing linearly, our percentage of observation used decrease exponentially.\n",
    "\n",
    "(e) Using the same logic we have(lets l = length of the cube)  \n",
    "\\begin{align}\n",
    "E(\\alpha) &= \\frac {1}{10} = \\frac {l^p}{1^p} \\\\\n",
    "l &= 10^{-1/p}\n",
    "\\end{align}\n",
    "\n",
    "Thus, for l = 1, 2,...,100:  \n",
    "\\begin{align}\n",
    "l &= 10^{-1/1} \\approx 0.1 \\\\\n",
    "l &= 10^{-1/2} \\approx 0.3126 \\\\\n",
    "l &= 10^{-1/100} \\approx 0.9772\n",
    "\\end{align}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Difference between LDA and QDA  \n",
    "(a) If Bayes decision boundary is linear, we expect QDA perform better on training set because \n",
    "of its flexibility which capture more noise in the data. However, on testing set, LDA would outperform\n",
    "QDA .  \n",
    "(b) If Bayes decision boundary is non-linear, we expect QDA to outperform LDA on both training set\n",
    "and test set. LDA cannot capture non-linear features of the data which increase more bias than the\n",
    "decrease variance.  \n",
    "(c) As we increase the size of our data set, the flexible model (in this case QDA) will slowly \n",
    "outperform the less flexible ones (LDA) in both training and testing data. In this scenario, test\n",
    "prediction accuracy of QDA relative to LDA will improve. Large data set will offset the variance in\n",
    "the bias-variance trade off!  \n",
    "(d) False. It is highly depend on the data set size. If we have a small dataset, the QDA decision boundary\n",
    "will output massive variance which overshadow any gain in bias."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. X1 = hours studied, X2 = undergrad GPA, and Y = receive an A. \n",
    "We fit a logistic regression and produce estimated coefficient,\n",
    "$\\hat{\\beta_0} = -6, \\hat{\\beta_1} = 0.05, \\hat{\\beta_2} = 1$\n",
    "\n",
    "(a) A student who study 40 hours and has undergrad GPA of 3.5 will have the prob of getting A:  \n",
    "\\begin{align}\n",
    "p(X) &= \\frac {exp(-6 + 0.05*40 + 3.5)} {1 + exp(-6 + 0.05*40 + 3.5)} \\\\\n",
    "p(X) &\\approx 0.3775 \n",
    "\\end{align}\n",
    "\n",
    "(b) For the same student to have 50% of getting an A:  \n",
    "\\begin{align}\n",
    "p(X) = 0.50 &= \\frac {exp(-6 + 0.05*X1 + 3.5)} {1 + exp(-6 + 0.05*X1 + 3.5)} \\\\\n",
    "1 &= exp(-2.5 + 0.05*X1) \\\\\n",
    "0 &= -2.5 + 0.05*X1 \\\\\n",
    "50 &= X1\n",
    "\\end{align}\n",
    "\n",
    "he has to study 50 hours!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "7. Yikes! This is more typing. Lets get started with the density function  \n",
    "Given \n",
    "\\begin{equation}\n",
    "\\mu_{yes} = 10, \\mu_{no} = 0, \\hat{\\sigma}^2 = 36, x=4, \\pi_{yes} = 0.8\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{align}\n",
    "p_k(x) &= \\frac {\\pi_k\n",
    "                \\frac {1} {\\sqrt{2 \\pi} \\sigma}\n",
    "                \\exp(- \\frac {1} {2 \\sigma^2} (x - \\mu_k)^2)\n",
    "               }\n",
    "               {\\sum {\n",
    "                \\pi_l\n",
    "                \\frac {1} {\\sqrt{2 \\pi} \\sigma}\n",
    "                \\exp(- \\frac {1} {2 \\sigma^2} (x - \\mu_l)^2)\n",
    "               }} \\\\\n",
    "p_{yes}(x) &= \\frac {\\pi_{yes}\n",
    "                \\frac {1} {\\sqrt{2 \\pi} \\sigma}\n",
    "                \\exp(- \\frac {1} {2 \\sigma^2} (x - \\mu_{yes})^2)\n",
    "               }\n",
    "               {\n",
    "                \\pi_{yes}\n",
    "                \\frac {1} {\\sqrt{2 \\pi} \\sigma}\n",
    "                \\exp(- \\frac {1} {2 \\sigma^2} (x - \\mu_{yes})^2) +\n",
    "                \\pi_{no}\n",
    "                \\frac {1} {\\sqrt{2 \\pi} \\sigma}\n",
    "                \\exp(- \\frac {1} {2 \\sigma^2} (x - \\mu_{no})^2)\n",
    "               } \\\\\n",
    "p_{yes}(x=4) &= \\frac {0.8\n",
    "                \\exp(- \\frac {1} {2 * 36^2} (4 - 10)^2)\n",
    "               }\n",
    "               {\n",
    "                0.8\n",
    "                \\exp(- \\frac {1} {2 * 36^2} (4 - 10)^2) +\n",
    "                0.2\n",
    "                \\exp(- \\frac {1} {2 * 36^2} (4 - 0)^2)\n",
    "               } \\\\\n",
    "p_{yes}(x=4) &\\approx 0.752              \n",
    "\\end{align}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "8. Logistic: training error = 20%, test error = 30%.  \n",
    "KNN with K=1 will always have training error = 0%. Thus the test error = 18 * 2 = 36%.  \n",
    "Thus, we will choose Logistic model in this case since it produced a lower test error. Training \n",
    "error should be ignored!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "9. Given odds = $\\frac {p_k} {1 - p_k}$ we can answer the following:  \n",
    "(a) On average, if $\\frac {p_{default}} {1-p_{default}}=0.37$ then $p_{default} \\approx 0.27$ \n",
    "or 27 people in 100.  \n",
    "(b) If $p_{default}=0.16$ then $\\frac {p_{default}} {1-p_{default}}= \\frac {0.16} {1-0.16}\\approx0.19$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
